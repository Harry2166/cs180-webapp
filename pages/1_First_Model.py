import streamlit as st
import os
import torch
import joblib
from transformers import AutoTokenizer, AutoModel 
from layout import prediction_classes, section 
import pandas as pd
import numpy as np

st.set_page_config(page_title="First Model", page_icon="üë§")

BASE_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
RF_MODEL_PATH = os.path.join(BASE_DIR, "models", "random_forest.pkl")
LR_MODEL_PATH = os.path.join(BASE_DIR, "models", "logistic_regression.pkl")

@st.cache_resource
def load_models():
    tokenizer = AutoTokenizer.from_pretrained("Harry2166/fine-tuned-climate-bert")
    bert_model = AutoModel.from_pretrained("Harry2166/fine-tuned-climate-bert")
    ml_model = joblib.load(LR_MODEL_PATH)
    return tokenizer, bert_model, ml_model 

def get_embedding(text, tokenizer, model):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True)
    inputs = {k: v.to(model.device) for k, v in inputs.items()}
    with torch.no_grad():
        outputs = model(**inputs)
    return outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()

tokenizer, model, ml = load_models()

section("# BERT embeddings w/ RandomForestClassfiers & Logistic Regression", "")

section("## Methodology", 
    """
    BERT stands for Bidirectional Encoder Representations from Transformers and it is a deep neural language model trained and fine-tuned on unlabeled text to predict what sentences or words would follow one another accurately. 

Alongside this, in a paper conducted by [Krishnan and Anoop (2023)](https://arxiv.org/abs/2310.08099), running BERT, (specifically the ClimateBERT model) and feeding its embeddings (vector representations that capture the semantic meaning of the text) to several ML algorithms was done in order to classify the sentiment of multiple tweets about climate change. 

Among the ML algorithms used, Random Forest classification produced the best results in terms of accuracy. Logistic Regression was also tested due to its capability of balancing the weights of the classes, which is useful due to the imbalance in the dataset.

For this option, we are opting to use the ‚Äúdistilbert-base-uncased‚Äù BERT model and fine tuning based on the Medium article by [Amit (2025)](https://medium.com/@heyamit10/fine-tuning-bert-for-sentiment-analysis-a-practical-guide-f3d9c9cac236).
    """
)

section("### Process", 
    """
    Detailed below is the general process for how this methodology would be done:

1. Preprocess text (tokenizing, padding and truncation of text, sampling labels, etc.)
2. Use Optuna to find the best fine-tuning settings (e.g., learning rate, batch size).
3. Train BERT on labeled sentiment data (the training data) and validating it using the developemtn set using the best parameters from Optuna.
4. Get embeddings from the fine-tuned BERT model via mean pooling.
5. Use these embeddings to train a Random Forest Classifier and Logistic Regression model for final sentiment prediction.
6. Classify sentiment using the ML models.
7. Select which model performed the best on the development set.

    """
)

section("## Results", "")
bert_params = "{'learning_rate': 4.194633467918101e-06,'batch_size': 8,'num_train_epochs': 15,'fp16': False,'weight_decay': 0.05}"
st.image(os.path.abspath("../cs180-webapp/images/bert_model_loss.png"), caption="Results of the Chosen BERT Model Parameters")
st.write(f"The parameters that were chosen for the fine-tuned BERT model that were generated by Optuna are the following: \n\n**{bert_params}**")
st.write("Epoch 4 was the selected epoch from the generated results because it contained the least validation loss out of all the trials & epochs. Additionally, it also hosted the best metrics out of the metrics shown in the screenshot.")

st.image(os.path.abspath("../cs180-webapp/images/rfc_results.png"), caption="Results of the Random Forest Classfier Model")
st.image(os.path.abspath("../cs180-webapp/images/lr_results.png"), caption="Results of the Logistic Regression Model")
st.write("Out of the two trained ML models, the model that produced the best results was the trained Logistic Regression model. Hence, this model was selected and can be tested below.")

section("## Try Our Model", "")
uploaded_file = st.file_uploader("Upload CSV", type="csv")
if uploaded_file:
    df = pd.read_csv(uploaded_file)
    st.write("Data Preview:")
    st.write(df.head())

    # Let user choose which column to use
    text_column = st.selectbox("Select the column containing the climate-related text", df.columns)

    if st.button("Run Classification"):
        try:
            # Get embeddings for each row
            embeddings = []
            for text in df[text_column].astype(str):
                embedding = get_embedding(text, tokenizer, model)
                embeddings.append(embedding)

            # Convert to array and predict
            X = np.vstack(embeddings)
            predictions = ml.predict(X)

            # Add results to dataframe
            df["Prediction"] = [prediction_classes[p] for p in predictions]

            st.success("Classification complete!")
            st.write(df)

            # Download link
            # csv = df.to_csv(index=False).encode('utf-8')
            # st.download_button("Download Results as CSV", csv, "classified_results.csv", "text/csv")

        except Exception as e:
            st.error(f"Error: {e}")

